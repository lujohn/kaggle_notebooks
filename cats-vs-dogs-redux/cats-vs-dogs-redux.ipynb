{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "\n",
    "import csv\n",
    "from scipy import misc\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Lambda, Dense, Conv2D, BatchNormalization, Activation, \\\n",
    "Flatten, MaxPooling2D, Dropout, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Set up directory structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats-vs-dogs-redux.ipynb  \u001b[34mtest\u001b[m\u001b[m/                     \u001b[34mtrain\u001b[m\u001b[m/\r\n",
      "sample_submission.csv     test.zip                  train.zip\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnlu/Desktop/deep_learning/kaggle/train\n"
     ]
    }
   ],
   "source": [
    "# Create a directory for validation images\n",
    "%mkdir validation\n",
    "%cd train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will move a random sample of 2000 images into the validation set. Additionally, we create a small set of samples (100 images) out of the validation set to be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_examples: 25000\n",
      "some_shuffled_examples: ['cat.10834.jpg' 'cat.10533.jpg' 'cat.11168.jpg']\n",
      "Moving 2500 to validation directory\n",
      "/Users/johnlu/Desktop/deep_learning/kaggle\n"
     ]
    }
   ],
   "source": [
    "g = glob('*.jpg')\n",
    "TRAINING_TO_VAL_FRAC = .9  # Train set size / Total number examples\n",
    "NUM_TR_EX = len(g)\n",
    "TRAINING_SIZE = int(np.ceil(NUM_TR_EX * TRAINING_TO_VAL_FRAC))\n",
    "VALIDATION_SIZE = NUM_TR_EX - TRAINING_SIZE\n",
    "\n",
    "print('num_examples: ' + str(NUM_TR_EX))\n",
    "perm = np.random.permutation(g)\n",
    "print('some_shuffled_examples: ' + str(perm[0:3]))\n",
    "\n",
    "print('Moving %d to validation directory' % VALIDATION_SIZE)\n",
    "for i in range(VALIDATION_SIZE):\n",
    "    os.rename(perm[i], '../validation/'+perm[i])\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnlu/Desktop/deep_learning/kaggle/validation\n",
      "/Users/johnlu/Desktop/deep_learning/kaggle\n",
      "cat.3269.jpg\n"
     ]
    }
   ],
   "source": [
    "# Create a sample directory\n",
    "SAMPLE_SIZE = 150\n",
    "%cd './validation/'\n",
    "g = glob('*.jpg')\n",
    "%cd '..'\n",
    "%mkdir sample\n",
    "perm = np.random.permutation(g)\n",
    "print(perm[0])\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    copyfile('./validation/'+perm[i], './sample/'+perm[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnlu/Desktop/deep_learning/kaggle/train\n",
      "/Users/johnlu/Desktop/deep_learning/kaggle\n",
      "/Users/johnlu/Desktop/deep_learning/kaggle/sample\n",
      "/Users/johnlu/Desktop/deep_learning/kaggle\n",
      "/Users/johnlu/Desktop/deep_learning/kaggle/validation\n",
      "/Users/johnlu/Desktop/deep_learning/kaggle\n"
     ]
    }
   ],
   "source": [
    "# For each directory (train/, validation/, and sample/), create separate subdirectories\n",
    "# for cats and dogs.\n",
    "%cd ./train/\n",
    "%mkdir cats dogs\n",
    "%mv cat.* ./cats\n",
    "%mv dog.* ./dogs\n",
    "%cd ..\n",
    "\n",
    "%cd ./sample/\n",
    "%mkdir cats dogs\n",
    "%mv cat.* ./cats\n",
    "%mv dog.* ./dogs\n",
    "%cd ..\n",
    "\n",
    "%cd ./validation/\n",
    "%mkdir cats dogs\n",
    "%mv cat.* ./cats\n",
    "%mv dog.* ./dogs\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnlu/Desktop/deep_learning/kaggle/cats-vs-dogs-redux/test\n",
      "mv: rename test to test/test: Invalid argument\n",
      "/Users/johnlu/Desktop/deep_learning/kaggle/cats-vs-dogs-redux\n"
     ]
    }
   ],
   "source": [
    "# Move all test examples into a single subdirectory of ./test (required by ImageDataGenerator.flow_from_directory())\n",
    "%cd ./test/\n",
    "%mkdir test\n",
    "%mv * test\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory Structure\n",
    "TRAIN_DIR = './train/'\n",
    "VAL_DIR = './validation/'\n",
    "TEST_DIR = './test/'\n",
    "SAMPLES_DIR = './sample/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Training Examples: 22500\n",
      "Number Validation Examples: 2500\n",
      "Number Testing Examples: 0\n"
     ]
    }
   ],
   "source": [
    "# Number of examples\n",
    "tr_size = len(glob(TRAIN_DIR+'cats/*.jpg')) + len(glob(TRAIN_DIR+'dogs/*.jpg'))\n",
    "val_size = len(glob(VAL_DIR+'cats/*.jpg')) + len(glob(VAL_DIR+'dogs/*.jpg'))\n",
    "test_size = len(glob(TEST_DIR+'*.jpg'))\n",
    "print('Number Training Examples: ' + str(tr_size))\n",
    "print('Number Validation Examples: ' + str(val_size))\n",
    "print('Number Testing Examples: ' + str(test_size))\n",
    "# Image Parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "N_CHANNELS = 3\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, N_CHANNELS)\n",
    "\n",
    "# Model Parameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "# To grab batches of images from directory. We have 3 generators in case we want to augment images\n",
    "# for, say, the test examples and not the validation examples.\n",
    "img_gen = ImageDataGenerator()\n",
    "sample_gen = ImageDataGenerator()\n",
    "val_data_gen = ImageDataGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    batches = img_gen.flow_from_directory(path, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=1)\n",
    "    return np.concatenate([batch for batch in batches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final layer activations\n",
    "Training would be much faster if we first save the final layer activations from VGG, which should have shape (n_examples, 1000). This last layer would then be fed into whatever model we decide on for finetuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "### TO IMPLEMENT ###\n",
    "# tr_final_act = get_data(TRAIN_DIR)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transfer Learning using VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((1,1,3))\n",
    "\n",
    "def preprocess_vgg(x):\n",
    "    x = x - vgg_mean     # subtract mean\n",
    "    return x[:,:,::-1]    # reverse axis bgr->rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_vgg_model(load_vgg_weights=True):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(preprocess_vgg, input_shape=input_shape))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3,3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3,3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3,3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3,3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    \n",
    "    # Load weights\n",
    "    if (load_vgg_weights):\n",
    "        model.load_weights('./vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "train_batches = img_gen.flow_from_directory(directory=TRAIN_DIR, \n",
    "                                            classes=['cats', 'dogs'], \n",
    "                                            target_size=(IMG_HEIGHT,IMG_WIDTH), \n",
    "                                            batch_size=BATCH_SIZE, \n",
    "                                            shuffle=True)                             \n",
    "\n",
    "val_batches = val_data_gen.flow_from_directory(directory=VAL_DIR, \n",
    "                                               classes=['cats', 'dogs'], \n",
    "                                               target_size=(IMG_HEIGHT,IMG_WIDTH), \n",
    "                                               batch_size=BATCH_SIZE) \n",
    "\n",
    "sample_batches = sample_gen.flow_from_directory(directory=SAMPLES_DIR, \n",
    "                                               classes=['cats', 'dogs'], \n",
    "                                               target_size=(IMG_HEIGHT,IMG_WIDTH), \n",
    "                                               batch_size=BATCH_SIZE) \n",
    "\n",
    "tr_imgs, tr_labels= next(train_batches)\n",
    "\n",
    "# Show one example\n",
    "plt.imshow(np.uint8(tr_imgs[13]))\n",
    "print(tr_labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_16 = create_vgg_model()\n",
    "\n",
    "# Remove last layer\n",
    "vgg_16.pop()\n",
    "for layer in vgg_16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add Dense layer\n",
    "vgg_16.add(Dense(2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Round Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== INITIAL ROUND =========================\n",
    "# Compile\n",
    "adamOpt = optimizers.Adam(0.001)\n",
    "vgg_16.compile(adamOpt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# img_gen augments data, so we will train on some constant factor \n",
    "# more than number training examples available\n",
    "AUGMENTATION_FACTOR = 1.2\n",
    "num_batches = tr_size // BATCH_SIZE  # number of batches (rounded down)\n",
    "steps_per_epoch = num_batches * AUGMENTATION_FACTOR\n",
    "\n",
    "vgg_16.fit_generator(train_batches, \n",
    "                     steps_per_epoch=steps_per_epoch,\n",
    "                     validation_data=val_batches, \n",
    "                     validation_steps=val_size//BATCH_SIZE, \n",
    "                     epochs=1)\n",
    "\n",
    "vgg_16.save_weights('first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Round 2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_weights(model, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print('loading weights from file: ' + filename)\n",
    "        model.load_weights(filename)\n",
    "        print('loaded weights')\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return 10e-4 / (10**epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ======================== ROUND2 =========================\n",
    "# Decrease Learning Rate by factor of 10\n",
    "adamOpt = optimizers.Adam(0.0001)\n",
    "vgg_16.compile(adamOpt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Adjust Model Parameters\n",
    "BATCH_SIZE = 32\n",
    "AUGMENTATION_FACTOR = 1.5\n",
    "num_batches = tr_size // BATCH_SIZE  # number of batches (rounded down)\n",
    "steps_per_epoch = num_batches * AUGMENTATION_FACTOR\n",
    "\n",
    "# Load weights from previous round(s)\n",
    "load_weights(vgg_16, './weights_1.hdf5')\n",
    "# ------------------- create callbacks (if any) ----------------\n",
    "callbacks = []\n",
    "## save_file = 'weights.{val_loss:.2f}.hdf5'\n",
    "## callbacks.append(ModelCheckpoint(save_file, monitor='val_loss', save_weights_only=True))\n",
    "callbacks.append(LearningRateScheduler(lr_schedule))\n",
    "    \n",
    "vgg_16.fit_generator(train_batches, steps_per_epoch=steps_per_epoch,\n",
    "                     validation_data=val_batches, validation_steps=val_size//BATCH_SIZE, \n",
    "                     epochs=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from file: ./weights_1.hdf5\n",
      "loaded weights\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "load_weights(vgg_16, './weights_1.hdf5')\n",
    "test_gen = ImageDataGenerator()\n",
    "test_batches = test_gen.flow_from_directory(directory=TEST_DIR, \n",
    "                                            class_mode=None,\n",
    "                                            target_size=(IMG_HEIGHT,IMG_WIDTH), \n",
    "                                            batch_size=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 14896s \n"
     ]
    }
   ],
   "source": [
    "preds = vgg_16.predict_generator(test_batches, steps=test_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# (Kaggle) Generate csv submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('eggs.csv', 'w') as csvfile:\n",
    "    fieldnames = ['first_name', 'last_name']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'first_name': 'Baked', 'last_name': 'Beans'})\n",
    "    writer.writerow({'first_name': 'Lovely', 'last_name': 'Spam'})\n",
    "    writer.writerow({'first_name': 'Wonderful', 'last_name': 'Spam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipped_preds = np.clip(preds, a_max=0.975, a_min=0.025)\n",
    "with open('submission.csv', 'w') as submission:\n",
    "    fieldnames = ['id', 'label']\n",
    "    writer = csv.DictWriter(submission, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, pred_one_hot in enumerate(clipped_preds):\n",
    "        pred_dog = pred_one_hot[1]\n",
    "        writer.writerow({'id': i+1, 'label': pred_dog})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
